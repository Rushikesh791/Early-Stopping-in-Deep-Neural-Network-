1.Early stopping:
Early stopping is a technique used in training deep neural networks (DNNs) to prevent overfitting and improve generalization performance. The basic idea behind early stopping is to monitor the performance of the model on a separate validation dataset during training. The validation dataset is distinct from the training dataset and is used to evaluate the model's performance on unseen data.

2.Dropout Layers:
Using dropout layers can indeed help improve the performance of a Deep Neural Network (DNN) by preventing overfitting, which in turn can lead to better generalization on unseen data.
